{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from tokenizers import BertWordPieceTokenizer\nfrom transformers import BertTokenizer\nfrom transformers import BertTokenizerFast\nimport argparse\nimport pandas as pd\nimport pickle\nimport jieba.analyse\nfrom tqdm import tqdm\nfrom transformers import GPT2TokenizerFast, GPT2LMHeadModel\nimport logging\nimport numpy as np\nimport json\nfrom tqdm import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-07T13:51:04.591099Z","iopub.execute_input":"2022-05-07T13:51:04.591374Z","iopub.status.idle":"2022-05-07T13:51:04.597240Z","shell.execute_reply.started":"2022-05-07T13:51:04.591345Z","shell.execute_reply":"2022-05-07T13:51:04.596414Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"import math\nimport time\nimport torch\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom datetime import datetime\nimport os\nfrom torch.utils.data import Dataset, DataLoader\nfrom os.path import join, exists\nfrom torch.nn import CrossEntropyLoss\nfrom tqdm import tqdm\nfrom torch.nn import DataParallel\nimport transformers\nimport pickle\nimport sys\n# from pytorchtools import EarlyStopping\nfrom sklearn.model_selection import train_test_split\n# from data_parallel import BalancedDataParallel\nfrom transformers import GPT2TokenizerFast, GPT2LMHeadModel, GPT2Config\nfrom transformers import BertTokenizerFast\nimport pandas as pd\nimport torch.nn.utils.rnn as rnn_utils\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2022-05-07T13:51:04.618674Z","iopub.execute_input":"2022-05-07T13:51:04.619065Z","iopub.status.idle":"2022-05-07T13:51:04.626927Z","shell.execute_reply.started":"2022-05-07T13:51:04.619029Z","shell.execute_reply":"2022-05-07T13:51:04.626178Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"def preprocess():\n    \"\"\"\n    对原始语料进行tokenize，将每段对话处理成如下形式：\"[CLS]utterance1[SEP]utterance2[SEP]utterance3[SEP]\"\n    \"\"\"\n    # 设置参数\n    config = {}\n    \n    config['vocab_path'] = '../input/gpt2base/gpt2通用中文模型/vocab.txt'\n    config['train_path'] = '../input/baike19/baike_qa_train.json'\n    config['save_path'] = './baike_qa_train.pkl'\n\n\n    # 初始化tokenizer\n    tokenizer = BertTokenizerFast(vocab_file=config['vocab_path'], sep_token=\"[SEP]\", pad_token=\"[PAD]\", cls_token=\"[CLS]\")\n    sep_id = tokenizer.sep_token_id\n    cls_id = tokenizer.cls_token_id\n    print(\"preprocessing data,data path:{}, save path:{}\".format(config['train_path'], config['save_path']))\n\n    train_data = []\n    for line in open(config['train_path'],'r'):\n        train_data.append(json.loads(line))\n\n\n    print(\"there are {} dialogue in dataset\".format(len(train_data)))\n\n    # 开始进行tokenize\n    # 保存所有的对话数据,每条数据的格式为：\"[CLS]utterance1[SEP]utterance2[SEP]utterance3[SEP]\"\n    dialogue_len = []  # 记录所有对话tokenize之后的长度，用于统计中位数与均值\n    dialogue_list = []\n    for data in tqdm(train_data):\n        title = data['title']\n        desc = data['desc']\n        if desc=='' or desc==None or desc==' ':\n            desc = title\n        answer = data['answer']\n        input_ids = [cls_id]  # 每个dialogue以[CLS]开头\n        for utterance in (desc,answer):\n            input_ids += tokenizer.encode(utterance, add_special_tokens=False)\n            input_ids.append(sep_id)  # 每个utterance之后添加[SEP]，表示utterance结束\n        dialogue_len.append(len(input_ids))\n        dialogue_list.append(input_ids)\n    len_mean = np.mean(dialogue_len)\n    len_median = np.median(dialogue_len)\n    len_max = np.max(dialogue_len)\n    with open(config['save_path'], \"wb\") as f:\n        pickle.dump(dialogue_list, f)\n    print('completed...')","metadata":{"execution":{"iopub.status.busy":"2022-05-07T13:51:04.638793Z","iopub.execute_input":"2022-05-07T13:51:04.639156Z","iopub.status.idle":"2022-05-07T13:51:04.652055Z","shell.execute_reply.started":"2022-05-07T13:51:04.639124Z","shell.execute_reply":"2022-05-07T13:51:04.651420Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"##处理train json文件\n##preprocess()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T13:51:04.657130Z","iopub.execute_input":"2022-05-07T13:51:04.657777Z","iopub.status.idle":"2022-05-07T13:51:04.665857Z","shell.execute_reply.started":"2022-05-07T13:51:04.657739Z","shell.execute_reply":"2022-05-07T13:51:04.664987Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"def collate_fn(batch):\n    input_ids = rnn_utils.pad_sequence(batch, batch_first=True, padding_value=0)###pad成相同长度\n    labels = rnn_utils.pad_sequence(batch, batch_first=True, padding_value=-100)\n    return input_ids, labels","metadata":{"execution":{"iopub.status.busy":"2022-05-07T13:51:04.675602Z","iopub.execute_input":"2022-05-07T13:51:04.675820Z","iopub.status.idle":"2022-05-07T13:51:04.680866Z","shell.execute_reply.started":"2022-05-07T13:51:04.675794Z","shell.execute_reply":"2022-05-07T13:51:04.679914Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"class MyDataset(Dataset):\n    \"\"\"\n\n    \"\"\"\n\n    def __init__(self, input_list, max_len):\n        self.input_list = input_list\n        self.max_len = max_len\n\n    def __getitem__(self, index):\n        input_ids = self.input_list[index]\n        input_ids = input_ids[:self.max_len]\n        input_ids = torch.tensor(input_ids, dtype=torch.long)\n        return input_ids\n\n    def __len__(self):\n        return len(self.input_list)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T13:51:04.695848Z","iopub.execute_input":"2022-05-07T13:51:04.696340Z","iopub.status.idle":"2022-05-07T13:51:04.702122Z","shell.execute_reply.started":"2022-05-07T13:51:04.696308Z","shell.execute_reply":"2022-05-07T13:51:04.701265Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"def load_dataset():\n    \"\"\"\n    加载训练集和验证集，这里直接划分问答的训练集为train/val，就不使用验证集了\n    \"\"\"\n    print(\"loading training dataset and validating dataset\")\n    train_path = '../input/baikepklsmall/baike_qa_train_small.pkl'\n    ##对话数目为1425170\n    with open(train_path, \"rb\") as f:\n        input_list = pickle.load(f)\n\n    # 划分训练集与验证集，这里数据量太大，这里先使用10w进行训练\n    val_num = 100000\n    input_list_train = input_list[:val_num]\n    input_list_val = input_list[val_num:val_num+2000]\n    # test\n    # input_list_train = input_list_train[:24]\n    # input_list_val = input_list_val[:24]\n    max_len = 300\n    train_dataset = MyDataset(input_list_train, max_len)\n    val_dataset = MyDataset(input_list_val, max_len)\n\n    return train_dataset, val_dataset","metadata":{"execution":{"iopub.status.busy":"2022-05-07T13:51:04.714357Z","iopub.execute_input":"2022-05-07T13:51:04.714683Z","iopub.status.idle":"2022-05-07T13:51:04.721235Z","shell.execute_reply.started":"2022-05-07T13:51:04.714651Z","shell.execute_reply":"2022-05-07T13:51:04.720291Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"def calculate_acc(logit, labels, ignore_index=-100):\n    logit = logit[..., :-1, :].contiguous().view(-1, logit.size(-1))\n    labels = labels[..., 1:].contiguous().view(-1)\n\n    _, logit = logit.max(dim=-1)  # 对于每条数据，返回最大的index\n    # 进行非运算，返回一个tensor，若labels的第i个位置为pad_id，则置为0，否则为1\n    non_pad_mask = labels.ne(ignore_index)\n    n_correct = logit.eq(labels).masked_select(non_pad_mask).sum().item()\n    n_word = non_pad_mask.sum().item()\n    return n_correct, n_word","metadata":{"execution":{"iopub.status.busy":"2022-05-07T13:51:04.734406Z","iopub.execute_input":"2022-05-07T13:51:04.734726Z","iopub.status.idle":"2022-05-07T13:51:04.743759Z","shell.execute_reply.started":"2022-05-07T13:51:04.734697Z","shell.execute_reply":"2022-05-07T13:51:04.742932Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"ignore_index = -100\ngradient_accumulation_steps = 8\nmax_grad_norm = 2\nbatch_size = 8\nnum_workers = 1\nsave_model_path = 'model'\npatience = 0\nepochs = 2\nlr = 1e-6\neps = 1.0e-09\nwarmup_steps = 4000\nvocab_path = '../input/gpt2base/gpt2通用中文模型/vocab.txt'\npretrained_model = '../input/gpt2base/gpt2通用中文模型'\nmodel_config = '../input/gpt2base/gpt2通用中文模型/config.json'\ndevice = 'cuda'","metadata":{"execution":{"iopub.status.busy":"2022-05-07T13:51:04.768684Z","iopub.execute_input":"2022-05-07T13:51:04.769436Z","iopub.status.idle":"2022-05-07T13:51:04.775769Z","shell.execute_reply.started":"2022-05-07T13:51:04.769394Z","shell.execute_reply":"2022-05-07T13:51:04.774854Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"def train_epoch(model, train_dataloader, optimizer, scheduler,epoch):\n    model.train()\n    device = 'cuda'\n    epoch_start_time = datetime.now()\n    total_loss = 0  # 记录下整个epoch的loss的总和\n\n    # epoch_correct_num:每个epoch中,output预测正确的word的数量\n    # epoch_total_num: 每个epoch中,output预测的word的总数量\n    epoch_correct_num, epoch_total_num = 0, 0\n\n    for batch_idx, (input_ids, labels) in enumerate(train_dataloader):\n        # 捕获cuda out of memory exception\n        try:\n            input_ids = input_ids.to(device)\n            labels = labels.to(device)\n            outputs = model.forward(input_ids, labels=labels)\n            logits = outputs.logits\n            loss = outputs.loss\n            loss = loss.mean()\n\n            # 统计该batch的预测token的正确数与总数\n            batch_correct_num, batch_total_num = calculate_acc(logits, labels, ignore_index=ignore_index)\n            # 统计该epoch的预测token的正确数与总数\n            epoch_correct_num += batch_correct_num\n            epoch_total_num += batch_total_num\n            # 计算该batch的accuracy\n            batch_acc = batch_correct_num / batch_total_num\n\n            total_loss += loss.item()\n            if gradient_accumulation_steps > 1:\n                loss = loss / gradient_accumulation_steps\n\n            loss.backward()\n            # 梯度裁剪\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n\n            # 进行一定step的梯度累计之后，更新参数\n            if (batch_idx + 1) % gradient_accumulation_steps == 0:\n                # 更新参数\n                optimizer.step()\n                # 更新学习率\n                scheduler.step()\n                # 清空梯度信息\n                optimizer.zero_grad()\n\n            if (batch_idx + 1) % 10 == 0:\n                print(\n                    \"batch {} of epoch {}, loss {}, batch_acc {}, lr {}\".format(\n                        batch_idx + 1, epoch + 1, loss.item() * gradient_accumulation_steps, batch_acc, scheduler.get_lr()))\n\n            del input_ids, outputs\n\n        except RuntimeError as exception:\n            if \"out of memory\" in str(exception):\n                print(\"WARNING: ran out of memory\")\n                if hasattr(torch.cuda, 'empty_cache'):\n                    torch.cuda.empty_cache()\n            else:\n                print(str(exception))\n                raise exception\n\n    # 记录当前epoch的平均loss与accuracy\n    epoch_mean_loss = total_loss / len(train_dataloader)\n    epoch_mean_acc = epoch_correct_num / epoch_total_num\n    print(\n        \"epoch {}: loss {}, predict_acc {}\".format(epoch + 1, epoch_mean_loss, epoch_mean_acc))\n\n    # save model\n    print('saving model for epoch {}'.format(epoch + 1))\n    model_path = join('model', 'epoch{}'.format(epoch + 1))\n    if not os.path.exists(model_path):\n        os.mkdir(model_path)\n    model_to_save = model.module if hasattr(model, 'module') else model\n    model_to_save.save_pretrained(model_path)\n    print('epoch {} finished'.format(epoch + 1))\n    epoch_finish_time = datetime.now()\n    print('time for one epoch: {}'.format(epoch_finish_time - epoch_start_time))\n\n    return epoch_mean_loss","metadata":{"execution":{"iopub.status.busy":"2022-05-07T13:51:04.805814Z","iopub.execute_input":"2022-05-07T13:51:04.806227Z","iopub.status.idle":"2022-05-07T13:51:04.823161Z","shell.execute_reply.started":"2022-05-07T13:51:04.806196Z","shell.execute_reply":"2022-05-07T13:51:04.822342Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"def validate_epoch(model, validate_dataloader,epoch):\n    print(\"start validating\")\n    model.eval()\n    device = 'cuda'\n    epoch_start_time = datetime.now()\n    total_loss = 0\n    # 捕获cuda out of memory exception\n    try:\n        with torch.no_grad():\n            for batch_idx, (input_ids, labels) in enumerate(validate_dataloader):\n                input_ids = input_ids.to(device)\n                labels = labels.to(device)\n                outputs = model.forward(input_ids, labels=labels)\n                logits = outputs.logits\n                loss = outputs.loss\n                loss = loss.mean()\n\n                total_loss += loss.item()\n                del input_ids, outputs\n\n            # 记录当前epoch的平均loss\n            epoch_mean_loss = total_loss / len(validate_dataloader)\n            print(\n                \"validate epoch {}: loss {}\".format(epoch+1, epoch_mean_loss))\n            epoch_finish_time = datetime.now()\n            print('time for validating one epoch: {}'.format(epoch_finish_time - epoch_start_time))\n            return epoch_mean_loss\n    except RuntimeError as exception:\n        if \"out of memory\" in str(exception):\n            print(\"WARNING: ran out of memory\")\n            if hasattr(torch.cuda, 'empty_cache'):\n                torch.cuda.empty_cache()\n        else:\n            print(str(exception))\n            raise exception\n","metadata":{"execution":{"iopub.status.busy":"2022-05-07T13:51:04.842374Z","iopub.execute_input":"2022-05-07T13:51:04.842714Z","iopub.status.idle":"2022-05-07T13:51:04.854102Z","shell.execute_reply.started":"2022-05-07T13:51:04.842674Z","shell.execute_reply":"2022-05-07T13:51:04.853245Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"def train(model, train_dataset, validate_dataset):\n    train_dataloader = DataLoader(\n        train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, collate_fn=collate_fn,\n        drop_last=True\n    )\n    validate_dataloader = DataLoader(validate_dataset, batch_size=batch_size, shuffle=True,\n                                     num_workers=num_workers, collate_fn=collate_fn, drop_last=True)\n#     early_stopping = EarlyStopping(patience, verbose=True, save_path=save_model_path)\n    t_total = len(train_dataloader) // gradient_accumulation_steps * epochs\n    optimizer = transformers.AdamW(model.parameters(), lr=lr, eps=eps)\n    # scheduler = transformers.WarmupLinearSchedule(optimizer, warmup_steps=args.warmup_steps, t_total=t_total)\n    scheduler = transformers.get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total\n    )\n\n    print('starting training')\n\n    # 用于记录每个epoch训练和验证的loss\n    train_losses, validate_losses = [], []\n    # 记录验证集的最小loss\n    best_val_loss = 10000\n    # 开始训练\n    for epoch in range(epochs):\n        # ========== train ========== #\n        train_loss = train_epoch(\n            model=model, train_dataloader=train_dataloader,\n            optimizer=optimizer, scheduler=scheduler, epoch=epoch)\n        train_losses.append(train_loss)\n\n        # ========== validate ========== #\n        validate_loss = validate_epoch(\n            model=model, validate_dataloader=validate_dataloader,epoch=epoch)\n        validate_losses.append(validate_loss)\n\n        # 保存当前困惑度最低的模型，困惑度低，模型的生成效果不一定会越好\n        if validate_loss < best_val_loss:\n            best_val_loss = validate_loss\n            print('saving current best model for epoch {}'.format(epoch + 1))\n            model_path = join(save_model_path, 'min_ppl_model'.format(epoch + 1))\n            if not os.path.exists(model_path):\n                os.mkdir(model_path)\n            model_to_save = model.module if hasattr(model, 'module') else model\n            model_to_save.save_pretrained(model_path)\n\n        #  如果patience=0,则不进行early stopping\n        if patience == 0:\n            continue\n#         early_stopping(validate_loss, model)\n#         if early_stopping.early_stop:\n#             print(\"Early stopping\")\n#             break\n    print('training finished')\n    print(\"train_losses:{}\".format(train_losses))\n    print(\"validate_losses:{}\".format(validate_losses))\n","metadata":{"execution":{"iopub.status.busy":"2022-05-07T13:51:04.878087Z","iopub.execute_input":"2022-05-07T13:51:04.878406Z","iopub.status.idle":"2022-05-07T13:51:04.891967Z","shell.execute_reply.started":"2022-05-07T13:51:04.878374Z","shell.execute_reply":"2022-05-07T13:51:04.889708Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"###训练代码\ntokenizer = BertTokenizerFast(vocab_file=vocab_path, sep_token=\"[SEP]\", pad_token=\"[PAD]\", cls_token=\"[CLS]\")\nif not os.path.exists(save_model_path):\n    os.mkdir(save_model_path)\nif pretrained_model:  # 加载预训练模型\n    model = GPT2LMHeadModel.from_pretrained(pretrained_model)\n    print(f'loaded model in {pretrained_model}')\nelse:  # 初始化模型\n    model_config = GPT2Config.from_json_file(model_config)\n    model = GPT2LMHeadModel(config=model_config)\nmodel = model.to(device)\nprint('model config:\\n{}'.format(model.config.to_json_string()))\n# 计算模型参数数量\nnum_parameters = 0\nparameters = model.parameters()\nfor parameter in parameters:\n    num_parameters += parameter.numel()\nprint('number of model parameters: {}'.format(num_parameters))\n###load dataset\ntrain_dataset, validate_dataset = load_dataset()\n###train\ntrain(model, train_dataset, validate_dataset)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T13:51:04.902701Z","iopub.execute_input":"2022-05-07T13:51:04.903124Z","iopub.status.idle":"2022-05-07T13:56:00.393837Z","shell.execute_reply.started":"2022-05-07T13:51:04.903095Z","shell.execute_reply":"2022-05-07T13:56:00.392683Z"},"trusted":true},"execution_count":52,"outputs":[]}]}